webスクレイピングのコード

import requests
from bs4 import BeautifulSoup
import time
from tqdm import tqdm
# url = 'https://www.walkerplus.com/event_list/ar1040/'

data = []

for page in range(1,7):
    
    url = 'https://www.walkerplus.com/event_list/ar1000/{}.html'.format(page)
    res = requests.get(url)
    soup_list_page = BeautifulSoup(res.text, 'html.parser')
    list_items = soup_list_page.find_all('div', attrs={'class': 'm-mainlist-item'})
    for list_item in list_items:
        
        
        datum = {}
        
        ##title取得
        item_title = list_item.find('span', attrs={'class': 'm-mainlist-item__ttl'}).text.replace('\u3000',' ')
        datum['title'] = item_title
        ##thumbnail取得
        item_thumbnail = list_item.find('img')['src']
        datum['thumbnail'] = item_thumbnail
        ##link取得
        item_link =  'https://www.walkerplus.com' + list_item.find('a')['href']
        datum['link'] = item_link
        ##tag取得
        item_tags = list_item.find_all('li', attrs={'class': 'm-mainlist-item__tagsitem'})
        item_tagList = []
        for item_tag in item_tags:
            item_tagList.append(item_tag.text)
        datum['tagList'] = item_tagList
        
        ##詳細ページ移動
        res_details = requests.get(item_link)
        soup_details_page = BeautifulSoup(res_details.text, 'html.parser')
        soup_details_main = soup_details_page.find('main').find_all('section')[0]
        ##contents取得
        item_contents = soup_details_main.find_all('p', attrs={'class': 'm-articleset--3__lead'})[0].text
        datum['contents'] = item_contents
        ##holdingPeriod取得
        item_holdingPeriod = soup_details_main.find('p', attrs={'class': 'm-detailheader__text'}).text.replace('\n','')
        datum['period'] = item_holdingPeriod
        
        ##startDate,endDate取得
        holdingPeriod = soup_details_main.find('p', attrs={'class': 'm-detailheader__text'})
        if holdingPeriod.find('span'):
            holdingPeriod.find('span').extract()
        else:
            print('no')
        holdingPeriod = holdingPeriod.text
        holdingPeriod = holdingPeriod.replace('・祝','').replace('\n', '')
        holdingPeriod = holdingPeriod.replace('上旬','1').replace('中旬','15').replace('下旬','27')
        holdingPeriod = holdingPeriod.replace('(日)','').replace('(月)','').replace('(火)','').replace('(水)','').replace('(木)','').replace('(金)','').replace('(土)','')
        holdingPeriod = holdingPeriod.replace('年','-').replace('月','-').replace('日','')  
            
            ##文字列の～から前を抽出
        start = holdingPeriod[:holdingPeriod.find('～')]
            ##文字列の～から後を抽出
        end = holdingPeriod[holdingPeriod.find('～') +1:]
        if '2020' in start:
            print('2020ok')
        elif '2021' in start:
            print('2021ok')
        else:
            start = '2021-' + start
        if start[6] == '-':
            start = start[0:5]+'0'+start[5:9]
        if len(start) == 9:
            start = start[0:8]+'0'+start[8]
        datum['startDate'] = start
        
        if '2020' in end:
            print('2020ok')
        elif '2021' in end:
            print('2021ok')
        else:
            end = '2021-' + end
        if end[6] == '-':
            end = end[0:5]+'0'+end[5:9]
        if len(end) == 9:
            end = end[0:8]+'0'+end[8]
        datum['endDate'] = end
        
        
        ##場所詳細ページ移動
        url_place = 'https://www.walkerplus.com'+ list_item.find('a', attrs={'class': 'm-mainlist-item-event__placelink'})['href']
        res_place = requests.get(url_place)
        soup_place_page = BeautifulSoup(res_place.text, 'html.parser')
        ##住所取得
        item_place = soup_place_page.find('table', attrs={'class': 'm-detailmain-table'}).find('p').text.replace('[地図]','')
        datum['streetAddress'] = item_place
        
        def get_lat_lon_from_address(address):
            """
            address_lにlistの形で住所を入れてあげると、latlonsという入れ子上のリストで緯度経度のリストを返す関数。
            >>>>get_lat_lon_from_address(['東京都文京区本郷7-3-1','東京都文京区湯島３丁目３０−１'])
            [['35.712056', '139.762775'], ['35.707771', '139.768205']]
            """
            url = 'http://www.geocoding.jp/api/'
            latlons = []
        #     for address in tqdm(address_l):
            payload = {'q': address}
            r = requests.get(url, params=payload)
            ret = BeautifulSoup(r.content,'lxml')
            if ret.find('error'):
                raise ValueError(f"Invalid address submitted. {address}")
            else:
                lat = ret.find('lat').string
                lon = ret.find('lng').string
                latlons.append([lat,lon])
                time.sleep(10)
            return [lat,lon]

        latlons = get_lat_lon_from_address(item_place);
        datum['longitudeLatitude'] = latlons
        datum['like'] = []
        
        data.append(datum)
data